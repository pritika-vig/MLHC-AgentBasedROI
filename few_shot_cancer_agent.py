# -*- coding: utf-8 -*-
"""few_shot_cancer_agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hSmjZ6x99flKC7vnFFORSeWOuv9Vxklk
"""

!pip install langchain openai pillow
!pip install anthropic
!pip install -U langchain-community
!pip install -U langchain-anthropic

!apt-get install -y openslide-tools
!pip install openslide-python

from google.colab import drive
drive.mount('/content/drive')

# Import standard libraries
import os
import ast
import pdb
import io
import json
import base64
from typing import Dict, List, Union
from collections import defaultdict
from pathlib import Path
import xml.etree.ElementTree as ET

# Import scientific computing and machine learning libraries
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

# Import image processing libraries
from PIL import Image as PILImage, ImageDraw, ImageFont
import openslide
from IPython.display import display

# Import External tools and APIs
import anthropic
from langchain.tools import tool

import random

# Path to shared drive
PATH_TO_SHARED_DRIVE = "/content/drive/My Drive/Colab Notebooks/Dataset/"
# Global cache for patches
patch_cache = {}

class ResNetModel(nn.Module):
    """Custom ResNet model for patch classification."""

    def __init__(self):
        super(ResNetModel, self).__init__()
        # Load pre-trained ResNet50
        base_model = models.resnet50(weights='IMAGENET1K_V1')

        # Extract feature layers
        layers = list(base_model.children())[:-1]
        self.features = nn.Sequential(*layers)

        # Freeze early layers
        self._freeze_early_layers()

        # Custom classifier
        self.classifier = self._create_classifier()

    def _freeze_early_layers(self):
        """Freeze early layers of the network."""
        ct = 0
        for child in self.features.children():
            ct += 1
            if ct < 7:
                for param in child.parameters():
                    param.requires_grad = False

    def _create_classifier(self):
        """Create a custom multi-layer classifier."""
        return nn.Sequential(
            nn.Linear(2048, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        """Forward pass through the network."""
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

def initialize_model(model_path):
    """Initialize and load the model."""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    model = ResNetModel()
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    print("Model loaded successfully")

    return model, device

def create_image_transform():
    """Create image transformation pipeline."""
    return transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])

# Initialize model and transform
MODEL_PATH = PATH_TO_SHARED_DRIVE + 'model_weights_only.pt'
model, device = initialize_model(MODEL_PATH)
transform = create_image_transform()

@tool
def region_of_interest(input: str) -> list:
    """
    Given a dict input with keys 'wsi_path' and optional 'annotation_path',
    sends WSI thumbnails to Claude and returns the top 2 tumor-suspect tiles as grid cell labels.
    Only displays the gridded image for the test file.
    """

    # === Parse input ===
    try:
        input_dict = ast.literal_eval(input)
        wsi_path = input_dict["wsi_path"]
        annotation_path = input_dict.get("annotation_path", None)
    except Exception as e:
        return [f"Error parsing input: {str(e)}"]

    def process_wsi_with_annotation(wsi_path, xml_path, grid_size_x=5, grid_size_y=5, highlight_cells=None, display_image=False):
        """
        Process WSI with annotations and create grid visualization
        highlight_cells: list of cell labels to highlight with red border (e.g., ['C2', 'D3'])
        display_image: whether to display the image
        """
        if highlight_cells is None:
            highlight_cells = []

        slide = openslide.OpenSlide(str(wsi_path))
        wsi_w, wsi_h = slide.dimensions
        n = 1024 * 1.5
        thumb_size = (n, int(n * wsi_h / wsi_w))
        thumb = slide.get_thumbnail(thumb_size)
        thumb_with_grid = thumb.copy()
        draw = ImageDraw.Draw(thumb_with_grid)

        x_scale = thumb_size[0] / wsi_w
        y_scale = thumb_size[1] / wsi_h

        cell_w = thumb_size[0] // grid_size_x
        cell_h = thumb_size[1] // grid_size_y

        cell_counts = defaultdict(int)
        row_labels = ["A", "B", "C", "D", "E"]
        col_labels = ["1", "2", "3", "4", "5"]

        # Process annotations if available
        if xml_path and Path(xml_path).exists():
            tree = ET.parse(str(xml_path))
            root = tree.getroot()
            annotations_tag = root.find("Annotations")
            if annotations_tag:
                for annotation in annotations_tag.findall("Annotation"):
                    coords_tag = annotation.find("Coordinates")
                    coords = []
                    if coords_tag:
                        for coord in coords_tag.findall("Coordinate"):
                            x = float(coord.attrib['X']) * x_scale
                            y = float(coord.attrib['Y']) * y_scale
                            coords.append((x, y))
                            col_idx = min(int(x // cell_w), grid_size_x - 1)
                            row_idx = min(int(y // cell_h), grid_size_y - 1)
                            label = f"{row_labels[row_idx]}{col_labels[col_idx]}"
                            cell_counts[label] += 1
                        if coords:
                            draw.polygon(coords, outline="red", width=3)

        # Draw grid and cell labels
        try:
            font = ImageFont.truetype("arial.ttf", size=50)
        except:
            font = ImageFont.load_default(size=100)

        # Draw grid lines
        for i in range(1, grid_size_x):
            draw.line([(i * cell_w, 0), (i * cell_w, thumb_size[1])], fill="black", width=6)
        for j in range(1, grid_size_y):
            draw.line([(0, j * cell_h), (thumb_size[0], j * cell_h)], fill="black", width=6)

        # Draw cell labels and highlight selected cells
        for row_idx in range(grid_size_y):
            for col_idx in range(grid_size_x):
                x = col_idx * cell_w
                y = row_idx * cell_h
                label = f"{row_labels[row_idx]}{col_labels[col_idx]}"

                # Highlight selected cells with red border
                if label in highlight_cells:
                    # Draw a thick red border around the selected cell
                    draw.rectangle([x, y, x + cell_w, y + cell_h],
                                  outline="red", width=10)

                    # Use a different color for the label to make it stand out
                    draw.text((x + 5, y + 5), label, fill="green", font=font)
                else:
                    draw.text((x + 5, y + 5), label, fill="black", font=font)

        # Display the thumbnail with grid and highlighted cells
        if display_image and highlight_cells:
            plt.figure(figsize=(15, 12))
            plt.imshow(thumb_with_grid)

            title = f"WSI with 5x5 Grid: {Path(wsi_path).stem}"
            title += f" - Selected cells: {', '.join(highlight_cells)}"
            plt.title(title)
            plt.axis('off')
            plt.show()

        # Encode image for Claude API
        buffered = io.BytesIO()
        thumb_with_grid.save(buffered, format="PNG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")

        top_cells = sorted(cell_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        return img_base64, [cell for cell, _ in top_cells]

    # Generate image + context for training examples (but don't display them)
    img_base64_011, top_cells_011 = process_wsi_with_annotation(wsi_path, annotation_path, display_image=False)

    # Use a second example to strengthen few-shot performance (again don't display)
    img_base64_022, top_cells_022 = process_wsi_with_annotation(
        str(wsi_path).replace("011", "022"),
        str(annotation_path).replace("011", "022"),
        display_image=False
    )

    example_prompt = (
        "You're a pathologist expert in recognizing tumor patterns in histological images. "
        "This is a thumbnail of a whole slide image (WSI) divided into a 5x5 grid. "
        "Columns are labeled 1 to 5 (left to right), and rows A to E (top to bottom). "
        "Red areas indicate tumor regions annotated by pathologists. "
        f"In this example, the 3 most tumor-rich tiles are: {', '.join(top_cells_011)}."
    )

    example_prompt_2 = (
        "You're a pathologist expert in recognizing tumor patterns in histological images. "
        "This is another WSI thumbnail. The most tumor-rich tiles are: " + ', '.join(top_cells_022) + "."
    )

    # Prediction (unlabeled image) - this is our test image, so we'll display it
    test_wsi_path = str(wsi_path).replace("011", "033")
    # First display it without highlighting
    img_base64_033, _ = process_wsi_with_annotation(
        test_wsi_path,
        xml_path=None,
        display_image=True
    )

    prediction_prompt = (
        "Now, as a pathologist, look at this unannotated thumbnail of a whole slide image (WSI) divided into a 5x5 grid "
        "(rows A–E, columns 1–5). "
        "Which 2 tiles are most likely to contain a tumor region? "
        "Only respond with a valid JSON object using this format:\n\n"
        "{ \"top_tiles\": [\"C2\", \"D1\"] }\n\n"
        "Do not provide any explanation or extra text."
    )

    # Claude message formatting
    def build_claude_image_message(base64_img, prompt_text):
        return {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/png",
                        "data": base64_img
                    }
                },
                {
                    "type": "text",
                    "text": prompt_text
                }
            ]
        }

    message_list = [
        build_claude_image_message(img_base64_011, example_prompt),
        build_claude_image_message(img_base64_022, example_prompt_2),
        build_claude_image_message(img_base64_033, prediction_prompt)
    ]

    # Call Claude
    client = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])
    response = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=512,
        messages=message_list
    )

    try:
        json_output = json.loads(response.content[0].text)
        predicted_cells = json_output["top_tiles"]

        # Display the prediction image with highlighted cells
        _, _ = process_wsi_with_annotation(
            test_wsi_path,
            xml_path=None,
            highlight_cells=predicted_cells,
            display_image=True
        )

        return predicted_cells
    except Exception as e:
        return [f"Error parsing Claude response: {str(e)}"]



@tool
def extract_thumbnail_coords_from_cells(cell_labels: Union[list, str], thumbnail_size: tuple = (1024, 779), grid_size: int = 5) -> dict:
    """
    Converts grid cell labels like ["C2", "D1"] into cell boundary coordinates in the thumbnail.
    Returns the top-left and bottom-right corners of each cell.
    """
    # Handle the case where cell_labels is a string representation of a list
    if isinstance(cell_labels, str):
        try:
            cell_labels = ast.literal_eval(cell_labels)
        except (ValueError, SyntaxError):
            return {"error": "Invalid input format. Expected a list of cell labels."}

    row_mapping = {"A": 0, "B": 1, "C": 2, "D": 3, "E": 4}
    col_mapping = {"1": 0, "2": 1, "3": 2, "4": 3, "5": 4}

    cell_w = thumbnail_size[0] // grid_size
    cell_h = thumbnail_size[1] // grid_size

    cell_boundaries = []

    for label in cell_labels:
        row_char, col_char = label[0], label[1]
        row = row_mapping.get(row_char.upper())
        col = col_mapping.get(col_char)

        if row is not None and col is not None:
            # Calculate top-left and bottom-right corners
            x1 = col * cell_w
            y1 = row * cell_h
            x2 = (col + 1) * cell_w
            y2 = (row + 1) * cell_h

            # Store as {cell_label: {top_left: (x1,y1), bottom_right: (x2,y2)}}
            cell_boundaries.append({
                "cell": label,
                "top_left": (x1, y1),
                "bottom_right": (x2, y2)
            })
        else:
            cell_boundaries.append({"cell": label, "error": "Invalid cell label"})

    return {"cell_boundaries": cell_boundaries}

@tool
def zoom(input: str) -> dict:
    """
    Zooms into a WSI based on cell boundaries and extracts exactly 10 patches
    spread across the selected cells.
    Input should be a dict: {"cell_boundaries": [...], "wsi_path": "..."}
    """
    input_dict = ast.literal_eval(input)
    cell_boundaries = input_dict["cell_boundaries"]
    wsi_path = input_dict["wsi_path"]

    thumbnail_width, thumbnail_height = 1024, 779
    patch_size = 512

    # Total number of patches to extract
    total_patches_needed = 10

    slide = openslide.OpenSlide(wsi_path)
    wsi_width, wsi_height = slide.dimensions

    scale_x = wsi_width / thumbnail_width
    scale_y = wsi_height / thumbnail_height

    patches = []
    images = []

    # Filter out cells with errors
    valid_cells = [cell for cell in cell_boundaries if "error" not in cell]
    num_cells = len(valid_cells)

    if num_cells == 0:
        return {"error": "No valid cells provided", "patches": [], "images": []}

    # Calculate how many patches to take from each cell
    patches_per_cell = total_patches_needed // num_cells
    remainder = total_patches_needed % num_cells

    # Process each cell
    for cell_idx, cell_info in enumerate(valid_cells):
        cell_label = cell_info["cell"]
        top_left = cell_info["top_left"]
        bottom_right = cell_info["bottom_right"]

        # Calculate cell boundaries in WSI coordinates
        wsi_x1 = int(top_left[0] * scale_x)
        wsi_y1 = int(top_left[1] * scale_y)
        wsi_x2 = int(bottom_right[0] * scale_x)
        wsi_y2 = int(bottom_right[1] * scale_y)

        # Calculate cell dimensions
        cell_width = wsi_x2 - wsi_x1
        cell_height = wsi_y2 - wsi_y1

        # Determine how many patches to extract from this cell
        patches_from_this_cell = patches_per_cell
        if cell_idx < remainder:
            patches_from_this_cell += 1

        # Create a grid for even distribution
        grid_size = max(1, int(patches_from_this_cell ** 0.5))

        # Calculate step sizes for even distribution
        step_x = cell_width / (grid_size + 1)
        step_y = cell_height / (grid_size + 1)

        # Extract evenly distributed patches
        patches_extracted = 0
        for i in range(1, grid_size + 1):
            for j in range(1, grid_size + 1):
                if patches_extracted >= patches_from_this_cell:
                    break

                # Calculate patch coordinates for even distribution
                x_center = wsi_x1 + i * step_x
                y_center = wsi_y1 + j * step_y

                # Adjust to get the top-left corner
                x_start = max(0, int(x_center - patch_size//2))
                y_start = max(0, int(y_center - patch_size//2))

                # Extract patch
                patch_img = slide.read_region((x_start, y_start), 0, (patch_size, patch_size))
                patch_img = patch_img.convert("RGB")
                display(patch_img)

                patch_id = f"Cell {cell_label} - Patch ({i},{j})"
                patches.append(patch_id)
                images.append(patch_img)
                patch_cache[patch_id] = patch_img

                patches_extracted += 1

        # If we couldn't fit enough patches in the grid, add random ones
        while patches_extracted < patches_from_this_cell:
            # Generate random coordinates within the cell
            rand_x = wsi_x1 + random.randint(0, max(1, cell_width - patch_size))
            rand_y = wsi_y1 + random.randint(0, max(1, cell_height - patch_size))

            # Extract patch
            patch_img = slide.read_region((rand_x, rand_y), 0, (patch_size, patch_size))
            patch_img = patch_img.convert("RGB")
            display(patch_img)

            patch_id = f"Cell {cell_label} - Random Patch {patches_extracted}"
            patches.append(patch_id)
            images.append(patch_img)
            patch_cache[patch_id] = patch_img

            patches_extracted += 1

    return {"patches": patches, "images": images, "num_patches": len(patches)}


@tool
def patch_classification(input: Union[str, List[str]]) -> Dict:
    """Classifies patches as cancerous or non-cancerous."""
    def parse_input(input_data):
        if isinstance(input_data, str):
            try:
                parsed = ast.literal_eval(input_data)
                return parsed if isinstance(parsed, list) else [parsed]
            except:
                return [input_data]
        return input_data if isinstance(input_data, list) else [input_data]

    try:
        patch_ids = parse_input(input)
        results = {}

        with torch.no_grad():
            for patch_id in patch_ids:
                img = patch_cache.get(patch_id)

                if img is None:
                    results[patch_id] = "Image not available in cache"
                    continue

                plt.figure(figsize=(6, 6))
                plt.imshow(img)
                plt.title(f"Patch: {patch_id}")
                plt.axis('off')
                plt.show()

                img_tensor = transform(img).unsqueeze(0).to(device)
                output = model(img_tensor)
                probability = float(output.item())

                classification = (
                    f"cancerous (confidence: {probability:.2f})"
                    if probability >= 0.5
                    else f"non-cancerous (confidence: {1-probability:.2f})"
                )

                results[patch_id] = classification

        return results

    except Exception as e:
        return {"error": f"Classification failed: {str(e)}"}

from langchain_anthropic import ChatAnthropic
from langchain.agents import initialize_agent, AgentType, Tool


os.environ["LANGCHAIN_TRACING_V2"] = "false"
#os.environ["ANTHROPIC_API_KEY"] = ""
llm = ChatAnthropic(model="claude-3-opus-20240229", temperature=0)

# Register tools
tools = [
    Tool.from_function(
        func=region_of_interest,
        name="region_of_interest",
        description="Takes a WSI path and optional annotation path, and returns a list of tumor-suspect cell labels (e.g., ['C2', 'D1'])."
    ),
    Tool.from_function(
        func=extract_thumbnail_coords_from_cells,
        name="extract_thumbnail_coords_from_cells",
        description="Takes a list of cell labels (like ['C2', 'D1']) and returns thumbnail coordinates [(x, y), ...] for zooming."
    ),
    Tool.from_function(
        func=zoom,
        name="zoom",
        description="Takes thumbnail coordinates and a WSI path, and returns patch images for each selected region."
    ),
    Tool.from_function(
        func=patch_classification,
        name="patch_classification",
        description="Takes a list of patch IDs (as strings) and returns cancer vs. non-cancer predictions for each patch."
    )
]

# Create the agent
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# Define paths to your data
from pathlib import Path
dataset_folder_path = PATH_TO_SHARED_DRIVE
tumor_011_thumbnail = str(Path(dataset_folder_path) / "thumbnails" / "tumor_011.jpg")
tumor_011_WSI = str(Path(dataset_folder_path) / "tumor" / "tumor_011.tif")
tumor_011_XML = str(Path(dataset_folder_path) / "tumor_annotations" / "tumor_011.xml")

# Agent query
query = f"""
Analyze the slide for cancer.

Use the region_of_interest tool with:
{{
  "wsi_path": "{tumor_011_WSI}",
  "annotation_path": "{tumor_011_XML}"
}}

Then use extract_thumbnail_coords_from_cells to convert those cell labels into cell boundaries.

Then use the zoom tool with:
{{"cell_boundaries": [...], "wsi_path": "{tumor_011_WSI}"}}

Finally, use patch_classification on the extracted patches.
"""

# Run the agent
response = agent.invoke(query)
print(response)